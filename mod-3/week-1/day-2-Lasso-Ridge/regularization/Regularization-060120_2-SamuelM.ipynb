{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Content__\n",
    "\n",
    "- [Objectives](#objectives)\n",
    "\n",
    "- [Review](#review)\n",
    "\n",
    "- [Regularization Techniques](#regularization_techniques)\n",
    "\n",
    "- [Questions](#questions)\n",
    "\n",
    "- [Appendix](#appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objectives \n",
    "<a name=\"objectives\"></a>\n",
    "\n",
    "- Understand what is regularization\n",
    "\n",
    "- Understand the effect of hyper-parameter $\\alpha$ in Ridge and Lasso.\n",
    "\n",
    "- Understand the similarities and differences between Lasso-Ridge-Linear models.\n",
    "\n",
    "- Apply Lasso and Ridge with sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "<a name=\"review\"></a>\n",
    "\n",
    "\n",
    "\n",
    "[__Overfitting - Underfitting__](https://github.com/gokererdogan/JaverianaMLCourse/blob/master/Lectures/05.pdf)\n",
    "\n",
    "<img src=\"underfitting_overfitting.png\" alt=\"Bias-Variance\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "[__Bias - Variance Trade-Off__](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
    "\n",
    "<img src=\"bias_variance_trade_off.png\" alt=\"Bias-Variance\" style=\"width: 400px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preliminaries - L1 and L2 Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose we have a vector:  \n",
    "\n",
    "$$\n",
    "  \\begin{align}\n",
    "    X &= \\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           x_{3} \\\\\n",
    "           x_{4}\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then the square of the \"__L2-norm__\" of this vector is given by:\n",
    "\n",
    "$$ \\| X \\|^{2}_{2} = x_{1}^{2} + x_{2}^{2} + x_{3}^{2} +x_{4}^{2} = \\sum_{i=1}^{4} x_{i}^{2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.12876483254676"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using numpy:\n",
    "import numpy as np\n",
    "X = np.array([12, -25, 10, -10])\n",
    "\n",
    "##L2 norm of X\n",
    "np.sqrt(sum(X*X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly the \"__L1-norm__\" of the X is given as:\n",
    "\n",
    "$$ \\| X \\|_{1} = \\lvert x_{1}\\rvert + \\lvert x_{2}\\rvert + \\lvert x_{3}\\rvert + \\lvert x_{4}\\rvert = \\sum_{i=1}^{4} \\lvert x_{i}\\rvert $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.449489742783178 4\n"
     ]
    }
   ],
   "source": [
    "X = np.array([2, 0, 1, -1])\n",
    "l1_norm = sum(np.abs(X))\n",
    "l1_norm\n",
    "l2_norm = np.sqrt(sum(X*X))\n",
    "\n",
    "print(l2_norm, l1_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In two dimensions: If $$\n",
    "  \\begin{align}\n",
    "    X &= \\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\text{(Lasso) ---->} \\| X \\|_{1} = \\lvert x_{1}\\rvert + \\lvert x_{2}\\rvert = \\sum_{i=1}^{2} \\lvert x_{i}\\rvert $$\n",
    "\n",
    "$$\\text{(Ridge) ---->} \\| X \\|_{2}^{2} = \\lvert x_{1}\\rvert^{2} + \\lvert x_{2}\\rvert^{2} = \\sum_{i=1}^{2} \\lvert x_{i}\\rvert^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"norms.png\" alt=\"Lasso-Lambda\" style=\"width: 400px;\" class = \"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Examples: Note that if we have two vectors $ \\begin{align}\n",
    "    X &= \\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           x_{3} \\\\\n",
    "           x_{4}\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}$   and $ \\begin{align}\n",
    "    Y &= \\begin{bmatrix}\n",
    "           y_{1} \\\\\n",
    "           y_{2} \\\\\n",
    "           y_{3} \\\\\n",
    "           y_{4}\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}$ we can subtract them and get a new vector: $ \\begin{align}\n",
    "    X - Y &= \\begin{bmatrix}\n",
    "           x_{1} - y_{1} \\\\\n",
    "           x_{2} - y_{2}\\\\\n",
    "           x_{3} - y_{3}\\\\\n",
    "           x_{4} - y_{4}\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}$. Then now we can calculate the __L1-norm__ of the new vector $X-Y$ as:\n",
    "  \n",
    "  $$ \\| X-Y \\|_{1} = \\lvert x_{1} - y_{1} \\rvert + \\lvert x_{2} - y_{2} \\rvert + \\lvert x_{3} - y_{3} \\rvert + \\lvert x_{4} - y_{4} \\rvert = \\sum\\limits_{i=1}^{4} \\lvert x_{i} - y_{i} \\rvert$$\n",
    "  \n",
    "  Similarly the __L2-norm__ of this vector is given by:\n",
    "  \n",
    "  $$ \\| X-Y \\|_{2}^{2} =  (x_{1} - y_{1})^{2} + (x_{2} - y_{2} )^{2} + ( x_{3} - y_{3} )^{2} + ( x_{4} - y_{4} )^{2} =\\sum\\limits_{i=1}^{4} ( x_{i} - y_{i} )^{2} $$\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Regression Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Linear Model\n",
    "\n",
    "$$ Y = w_{0} + w_{1}X_1 + w_{2}X_{2} + \\cdots + w_{p}X_{p} + \\varepsilon $$\n",
    "\n",
    " - We train model to understand the paramaters $w_{i}$ (Coefficients) \n",
    "\n",
    " - $X_{i}$ 's are columns of the datasets (Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Once the coefficients are estimated (learned): We can make predictions for an observation (row) in our dataset:\n",
    "\n",
    " $$ \\hat{y}_{i} =  \\hat{w}_{0} + \\hat{w}_{1}X_{i1} + \\hat{w}_{2}X_{i2} + \\cdots + \\hat{w}_{p}X_{ip}$$\n",
    " \n",
    " - $\\hat{w}_{i}$'s are estimated coefficients\n",
    " - $X_{i1}, X_{i2}, \\cdots, X_{ip}$ - i'th row in the dataset (with p-columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    " Therefore for each observations (rows) errors are given by:\n",
    " \n",
    " $$ e_{i} = y_{i} - \\hat{y}_{i} $$\n",
    " \n",
    " As a result, the Residual Sum of Squares can be expressed as:\n",
    " \n",
    " $$ RSS(\\hat{w}_{0}, \\cdots, \\hat{w}_{p}) = \\sum\\limits_{i=0}^{N} e_{i}^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Usually the cost functions are denoted with the capital letter $J$\n",
    " \n",
    " \n",
    " $$ J(\\hat{w}_{0}, \\cdots, \\hat{w}_{p}) = \\sum\\limits_{i=0}^{N} (y_{i} - \\hat{w}_{0} - \\hat{w}_{1}X_{i1} - \\hat{w}_{2}X_{i2} - \\cdots - \\hat{w}_{p}X_{i_p})^{2} $$\n",
    " \n",
    " And this equation can be written in short hand as:\n",
    " \n",
    " $$ J(\\boldsymbol{w}) = \\| \\boldsymbol{Y} - X \\hat{w} \\|_{2}^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ridge regularization\n",
    "\n",
    "Instead of minimizing $J(w)$ (least squares method), we will minimize:\n",
    "\n",
    "$$ J_{\\alpha}(\\boldsymbol{w}) = J(\\boldsymbol{w}) + \\alpha\\sum_{i=1}^{p} w_{i}^{2} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Lasso regularization\n",
    "\n",
    "Instead of minimizing $J(\\boldsymbol{w})$, we will minimize:\n",
    "\n",
    "$$ J_{\\alpha}(\\boldsymbol{w}) = J(\\boldsymbol{w}) + \\alpha\\sum_{i=1}^{p}| w_{i} | $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularization Techniques\n",
    "\n",
    "<a name=\"regularization_techniques\"></a>\n",
    "\n",
    "\n",
    "- Why?\n",
    "\n",
    "    - Reduces complexity\n",
    "    \n",
    "    - Reduce the chance of overfitting.\n",
    "    \n",
    "    - Reduces model's variance at the expense of introducing small bias\n",
    "    \n",
    "    - Increases model's interprettability.\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "import pandas as pd\n",
    "df = pd.read_csv('../data/Credit.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.891</td>\n",
       "      <td>3606</td>\n",
       "      <td>283</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>106.025</td>\n",
       "      <td>6645</td>\n",
       "      <td>483</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Asian</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104.593</td>\n",
       "      <td>7075</td>\n",
       "      <td>514</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>148.924</td>\n",
       "      <td>9504</td>\n",
       "      <td>681</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55.882</td>\n",
       "      <td>4897</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Income  Limit  Rating  Cards  Age  Education  Gender Student Married  \\\n",
       "1   14.891   3606     283      2   34         11    Male      No     Yes   \n",
       "2  106.025   6645     483      3   82         15  Female     Yes     Yes   \n",
       "3  104.593   7075     514      4   71         11    Male      No      No   \n",
       "4  148.924   9504     681      3   36         11  Female      No      No   \n",
       "5   55.882   4897     357      2   68         16    Male      No     Yes   \n",
       "\n",
       "   Ethnicity  Balance  \n",
       "1  Caucasian      333  \n",
       "2      Asian      903  \n",
       "3      Asian      580  \n",
       "4      Asian      964  \n",
       "5  Caucasian      331  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see the head of the dataset\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Brief Investigation of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 400 entries, 1 to 400\n",
      "Data columns (total 11 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Income     400 non-null    float64\n",
      " 1   Limit      400 non-null    int64  \n",
      " 2   Rating     400 non-null    int64  \n",
      " 3   Cards      400 non-null    int64  \n",
      " 4   Age        400 non-null    int64  \n",
      " 5   Education  400 non-null    int64  \n",
      " 6   Gender     400 non-null    object \n",
      " 7   Student    400 non-null    object \n",
      " 8   Married    400 non-null    object \n",
      " 9   Ethnicity  400 non-null    object \n",
      " 10  Balance    400 non-null    int64  \n",
      "dtypes: float64(1), int64(6), object(4)\n",
      "memory usage: 37.5+ KB\n"
     ]
    }
   ],
   "source": [
    "## checking for missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Caucasian           199\n",
       "Asian               102\n",
       "African American     99\n",
       "Name: Ethnicity, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## let's see the number of categories in ethnicity\n",
    "df.Ethnicity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Female    207\n",
       " Male     193\n",
       "Name: Gender, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     360\n",
       "Yes     40\n",
       "Name: Student, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Student.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yes    245\n",
       "No     155\n",
       "Name: Married, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Married.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns = \"Balance\"),\n",
    "                                                df.Balance, \n",
    "                                                stratify = df.Married,\n",
    "                                                random_state =42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding the Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## instantiate ohe\n",
    "## note that in this case we know the categories very well.\n",
    "## import\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "## instantiate\n",
    "ohe = OneHotEncoder(categories= [[\"Caucasian\", \"Asian\", \"African American\"], [\"Female\", \" Male\"], [\"No\", \"Yes\"], [\"Yes\", \"No\"]],\n",
    "                    drop = [\"Caucasian\", \"Female\", \"No\", \"No\"], sparse = False,)\n",
    "\n",
    "## fit model (learn categories in train and plan how to convert them one hot encoded columns)\n",
    "ohe.fit(X_train[['Ethnicity', \"Gender\", \"Student\", \"Married\"]])\n",
    "## transform X_train data. Result is a numpy array.\n",
    "categorical_train = ohe.transform(X_train[['Ethnicity', \"Gender\", \"Student\", \"Married\"]])\n",
    "## concact the categorical variables with the rest of the data\n",
    "train = np.hstack((X_train.select_dtypes(exclude = \"object\" ), categorical_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 11)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note that we should make the similar transformation to the test data too.\n",
    "## Remark: Never fit a transformer to your test data. Only transform!!\n",
    "categorical_test = ohe.transform(X_test[['Ethnicity', \"Gender\", \"Student\", \"Married\"]])\n",
    "test = np.hstack((X_test.select_dtypes(exclude = \"object\"), categorical_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0_Asian', 'x0_African American', 'x1_ Male', 'x2_Yes', 'x3_Yes'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Linear Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import \n",
    "from sklearn.linear_model import LinearRegression\n",
    "## instantiate\n",
    "lr = LinearRegression()\n",
    "## fit \n",
    "lr.fit(train, y_train)\n",
    "## predict\n",
    "y_train_pred = lr.predict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -7.691,   0.181,   1.227,  15.466,  -0.671,  -1.419,   7.498,\n",
       "        -2.283,   8.193, 433.656,  -5.428])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True, precision=3)\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9532149163845625"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## R_squared score\n",
    "lr.score(train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import\n",
    "from sklearn.linear_model import Ridge\n",
    "## instantiate\n",
    "ridge = Ridge(alpha = 10, normalize = True) ## note that we should normalize/standardize data if you are using regularization\n",
    "## fit\n",
    "ridge.fit(train, y_train)\n",
    "## predict\n",
    "ridge_train_pred = ridge.predict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.421,  0.014,  0.207,  2.757, -0.059,  0.147,  1.417,  5.896,\n",
       "       -3.954, 40.557,  0.194])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## coefficients\n",
    "ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2595672954223692"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## score\n",
    "ridge.score(train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.   ,  0.   ,  0.229,  0.   , -0.   ,  0.   ,  0.   ,  0.   ,\n",
       "       -0.   ,  0.   ,  0.   ])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import\n",
    "from sklearn.linear_model import Lasso\n",
    "## instantiate\n",
    "lasso = Lasso(alpha= 20, normalize= True)\n",
    "## fit\n",
    "lasso.fit(train, y_train)\n",
    "## predict\n",
    "lasso_train_pred = lasso.predict(train)\n",
    "## coefficients\n",
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12656179711435056"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## score\n",
    "\n",
    "lasso.score(train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Important Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "\n",
    "\n",
    "__Q.__ Should I do normalization for Lasso or Ridge?\n",
    "\n",
    "A. Yes? Why?\n",
    "\n",
    "__Q.__ When we know that Ridge and Lasso is better than vanilla linear regression?\n",
    "\n",
    "A. High variation in your model --> Colinearity and too many variables.\n",
    "\n",
    "__Q.__ How do we know whether we should choose Lasso or Ridge?\n",
    "\n",
    "A. Most of the time they perform very similar but Lasso has the feature selection property, ridge doesn't have this.\n",
    "\n",
    "__Q:__ How do we choose $\\lambda$?\n",
    "\n",
    "A. [sklearn gridsearch](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) for small models or random grid search for bigger models.\n",
    "\n",
    "Or check-out: \n",
    "\n",
    "[Sklearn - LassoLarsIC](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsIC.html)\n",
    "\n",
    "[Sklearn - LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "## instantiate\n",
    "scaler = StandardScaler()\n",
    "## fit\n",
    "scaler.fit(train)\n",
    "## transform\n",
    "scaled_train = scaler.transform(train) \n",
    "## Apply to test: !! Don't fit only transform\n",
    "scaled_test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing the effect of Regularization in Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding higher order terms:  (320, 4368) (80, 4368)\n"
     ]
    }
   ],
   "source": [
    "## import\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "## instantiate -- with desired degree. \n",
    "poly = PolynomialFeatures(degree=5)\n",
    "## fit\n",
    "poly.fit(scaled_train)\n",
    "## transform\n",
    "Xp_train = poly.transform(scaled_train)\n",
    "## apply to test: !! Don't fit only transform\n",
    "Xp_test = poly.transform(scaled_test)\n",
    "print('After adding higher order terms: ', Xp_train.shape, Xp_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuel/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 793460.4841875316, tolerance: 5257.7543359375\n",
      "  positive)\n",
      "/Users/samuel/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1722184.8873494123, tolerance: 5440.306824609375\n",
      "  positive)\n",
      "/Users/samuel/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1774828.4260413917, tolerance: 5469.784321484375\n",
      "  positive)\n",
      "/Users/samuel/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1324537.178013158, tolerance: 5191.286458984375\n",
      "  positive)\n",
      "/Users/samuel/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1228092.1018379226, tolerance: 5195.538730859375\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## import\n",
    "from sklearn.model_selection import cross_validate\n",
    "## note that cross-validate is a function so we don't need to instantiate it\n",
    "\n",
    "\n",
    "## chose a model for validate\n",
    "lasso = Lasso(alpha = 10)\n",
    "\n",
    "cv = cross_validate(estimator=lasso, X=Xp_train, y=y_train, cv = 5, return_estimator= True, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of cv object is <class 'dict'>\n",
      "Keys of cv object is dict_keys(['fit_time', 'score_time', 'estimator', 'test_score', 'train_score'])\n"
     ]
    }
   ],
   "source": [
    "## cv is an dict object\n",
    "print(\"Type of cv object is\", type(cv))\n",
    "print(\"Keys of cv object is\", cv.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.973 0.976 0.974 0.974 0.972]\n",
      "[0.719 0.689 0.874 0.961 0.946]\n"
     ]
    }
   ],
   "source": [
    "print(cv['train_score'])\n",
    "\n",
    "## note that even though the name is test_score this is in fact validation score\n",
    "print(cv['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_experiment(X, y, num_iter = 5, \n",
    "                     models = ['ols', 'ridge', 'lasso'], alpha= 10, \n",
    "                     complexity = 'simple', degree = 3):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    _________________________\n",
    "    num_iter: int, number of times fit the models to test data each time with different splitting. \n",
    "    note that for each iteration we split the data random train and test parts.\n",
    "    models: list, list of models that we want to use. Options are 'ols' for simple linear regression\n",
    "    'ridge' for ridge regression and 'lasso' for lasso regression.\n",
    "    alpha: float, alpha parameter for ridge and lasso algorithms. Recall that higher values of alpha \n",
    "    leads to more regularization.\n",
    "    complexity: str, either 'simple' or 'polynomial'. We either use the original dataset or \n",
    "    a dataset with polynomial powers generated. \n",
    "    degree: int, if complexity is polynomial then degree is the degrees of polynomials to be generated.\n",
    "    return: dict, it returns a dictionary with trained models as values and the 'complexity' parameters as keys.\n",
    "    \"\"\"\n",
    "    \n",
    "    x_axis = np.arange(num_iter)\n",
    "    y_ols_test = []\n",
    "    y_lasso_test = []\n",
    "    y_ridge_test = []\n",
    "    sample_models = {}\n",
    "    for i in range(num_iter):\n",
    "        \n",
    "        if complexity == 'simple':\n",
    "            ## split train_test \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "        elif complexity == 'polynomial':\n",
    "            ## Create higher order terms\n",
    "            poly = PolynomialFeatures(degree=degree)\n",
    "            Xp = poly.fit_transform(X)\n",
    "            ## test-train split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(Xp, y, test_size = 0.2)\n",
    "\n",
    "\n",
    "        ## Standard scale mean = 0, variance = 1\n",
    "        sd = StandardScaler()\n",
    "\n",
    "        sd.fit(X_train)\n",
    "\n",
    "        X_train = sd.transform(X_train)\n",
    "\n",
    "        X_test = sd.transform(X_test)\n",
    "\n",
    "        ## Be careful about the leakage\n",
    "\n",
    "        ## Vanilla model\n",
    "        if 'ols' in models:\n",
    "            lr = LinearRegression()\n",
    "\n",
    "            lr.fit(X_train, y_train)\n",
    "            \n",
    "            sample_models['ols'] = lr\n",
    "\n",
    "            test_score = lr.score(X_test, y_test)\n",
    "            train_score = lr.score(X_train, y_train)\n",
    "\n",
    "            y_ols_test.append(test_score)\n",
    "\n",
    "    #       print('test score OLS is %.2f and train score is %.2f'%(test_score, train_score))\n",
    "\n",
    "        if 'ridge' in models:\n",
    "            ## Ridge in the simple setting\n",
    "            ridge = Ridge(alpha = alpha, max_iter= 10000)\n",
    "            ridge.fit(X_train, y_train)\n",
    "            sample_models['ridge'] = ridge\n",
    "            y_ridge_test.append(ridge.score(X_test, y_test))\n",
    "    #         print('test score Ridge is %.2f and train score is %.2f'%(ridge.score(X_test, y_test),\n",
    "    #                                                             ridge.score(X_train, y_train)))\n",
    "\n",
    "        if 'lasso' in models:\n",
    "            ## Lasso in the simple setting\n",
    "            lasso = Lasso(alpha = alpha, max_iter= 10000)\n",
    "\n",
    "            lasso.fit(X_train, y_train)\n",
    "            \n",
    "            sample_models['lasso'] = lasso\n",
    "            \n",
    "            y_lasso_test.append(lasso.score(X_test, y_test))\n",
    "    #       print('test score Lasso is %.2f and train score is %.2f'%(lasso.score(X_test, y_test),\n",
    "    #                                                             lasso.score(X_train, y_train)))\n",
    "\n",
    "        i+=1\n",
    "    if 'ols' in models:\n",
    "        plt.plot(y_ols_test, label = 'ols')\n",
    "    if 'ridge' in models:\n",
    "        plt.plot(y_ridge_test, label = 'ridge')\n",
    "    if 'lasso' in models:\n",
    "        plt.plot(y_lasso_test, label = 'lasso')\n",
    "    plt.ylabel('R2 test score')\n",
    "    plt.xlabel('number of iterations')\n",
    "    plt.ylim((0.50, 0.99))\n",
    "    \n",
    "    plt.legend()\n",
    "    return sample_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_models = model_experiment(scaled_train, y_train, num_iter=10, alpha = 100,\n",
    "                                   models = ['ols', 'ridge', 'lasso'], \n",
    "                                   complexity= 'polynomial', degree = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn__\n",
    "\n",
    "- Try different values for alpha --> report your observations\n",
    "\n",
    "- Change complexity = 'polynomial' and observe the change in the variance of the models. \n",
    "\n",
    "- Report your observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.   ,  -0.   ,   0.   , 282.634,   0.   ,  -0.   ,   0.   ,\n",
       "         0.   ,   0.   ,  -0.   ,   0.   ,  -0.   ,  -0.   ,  -0.   ,\n",
       "        -0.   ,   0.   ,  -0.   ,   0.   ,   0.   ,   0.   ,  -0.   ,\n",
       "         0.   ,  -0.   ,   0.   ,   0.   ,   0.   ,  -0.   ,   0.   ,\n",
       "         0.   ,   0.   ,  -0.   ,   0.   ,  -0.   ,   0.   ,   0.   ,\n",
       "        -0.   ,   0.   ,   0.   ,   0.   ,  -0.   ,   0.   ,  -0.   ,\n",
       "         0.   ,  -0.   ,  -0.   ,  -0.   ,   0.   ,  -0.   ,  -0.   ,\n",
       "         0.   ,  -0.   ,  -0.   ,   0.   ,   0.   ,  -0.   ,  -0.   ,\n",
       "        -0.   ,   0.   ,   0.   ,  -0.   ,  -0.   ,   0.   ,   0.   ,\n",
       "         0.   ,  -0.   ,   0.   ,   0.   ,  -0.   ,   0.   ,   0.   ,\n",
       "         0.   ,   0.   ,  -0.   ,  -0.   ,  -0.   ,   0.   ,  -0.   ,\n",
       "         0.   ,  -0.   ,   0.   ,   0.   ,   0.   ,  -0.   ,   0.   ,\n",
       "         0.   ,   0.   ,  -0.   ,   0.   ,  -0.   ,   0.   ,   0.   ,\n",
       "         0.   ,  -0.   ,   0.   ,   0.   ,   0.   ,  -0.   ,   0.   ,\n",
       "        -0.   ,   0.   ,   0.   ,  -0.   ,   0.   ,   0.   ,   0.   ,\n",
       "        -0.   ,   0.   ,  -0.   ,   0.   ,   0.   ,  -0.   ,  -0.   ,\n",
       "         0.   ,  -0.   ,  -0.   ,   0.   ,  -0.   ,  -0.   ,  -0.   ,\n",
       "         0.   ,   0.   ,   0.   ,  -0.   ,  -0.   ,  -0.   ,  -0.   ,\n",
       "        -0.   ,   0.   ,   0.   ,  -0.   ,  -0.   ,  -0.   ,  -0.   ,\n",
       "         0.   ,  -0.   ,   0.   ,   0.   ,  -0.   ,  -0.   ,  -0.   ,\n",
       "         0.   ,  -0.   ,  -0.   ,  -0.   ,   0.   ,   0.   ,   0.   ,\n",
       "        -0.   ,   0.   ,   0.   ,   0.   ,  -0.   ,   0.   ,  -0.   ,\n",
       "         0.   ,   0.   ,  -0.   ,   0.   ,   0.   ,   0.   ,  -0.   ,\n",
       "         0.   ,  -0.   ,   0.   ,   0.   ,  -0.   ,  -0.   ,   0.   ,\n",
       "        -0.   ,  -0.   ,   0.   ,   0.   ,  -0.   ,  -0.   ,   0.   ,\n",
       "         0.   ,  -0.   ,  -0.   ,   0.   ,  -0.   ,  -0.   ,  -0.   ,\n",
       "         0.   ,   0.   ,   0.   ,  -0.   ,  -0.   ,   0.   ,   0.   ,\n",
       "         0.   ,   0.   ,   0.   ,  -0.   ,   0.   ,  -0.   ,  -0.   ,\n",
       "         0.   ,  -0.   ,   0.   ,   0.   ,   0.   ,  -0.   ,   0.   ,\n",
       "         0.   ,   0.   ,  -0.   ,   0.   ,  -0.   ,   0.   ,   0.   ,\n",
       "        -0.   ,  -0.   ,   0.   ,  -0.   ,  -0.   ,   0.   ,   0.   ,\n",
       "        -0.   ,  -0.   ,   0.   ,   0.   ,  -0.   ,  -0.   ,   0.   ,\n",
       "        -0.   ,  -0.   ,  -0.   ,   0.   ,   0.   ,   0.   ,  -0.   ,\n",
       "        -0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,  -0.   ,\n",
       "         0.   ,  -0.   ,  -0.   ,   0.   ,  -0.   ,   0.   ,   0.   ,\n",
       "        -0.   ,  -0.   ,  -0.   ,  -0.   ,   0.   ,   0.   ,  -0.   ,\n",
       "         0.   ,  -0.   ,  -0.   ,   0.   ,   0.   ,  -0.   ,  -0.   ,\n",
       "         0.   ,  -0.   ,  -0.   ,   0.   ,  -0.   ,  -0.   ,  -0.   ,\n",
       "         0.   ,  -0.   ,  -0.   ,   0.   ,   0.   ,  -0.   ,   0.   ,\n",
       "        -0.   ,   0.   ,  -0.   ,   0.   ,  -0.   ,   0.   ,   0.   ,\n",
       "        -0.   ,  -0.   ,   0.   ,   0.   ,  -0.   ,   0.   ,  -0.   ,\n",
       "        -0.   ,   0.   ,  -0.   ,   0.   ,   0.   ,   0.   ,  -0.   ,\n",
       "         0.   ,   0.   ,   0.   ,   0.   ,  -0.   ,   0.   ,  -0.   ,\n",
       "        -0.   ,  -0.   ,   0.   ,   0.   ,  -0.   ,  -0.   ,  -0.   ,\n",
       "        -0.   ,   0.   ,   0.   ,  -0.   ,   0.   ,  -0.   ,   0.   ,\n",
       "        -0.   ,  -0.   ,   0.   ,   0.   ,  -0.   ,   0.   ,   0.   ,\n",
       "        -0.   ,   0.   ,   0.   ,   0.   ,   0.   ,  -0.   ,  -0.   ,\n",
       "         0.   ,   0.   ,  -0.   ,   0.   ,  -0.   ,  -0.   ,  -0.   ,\n",
       "        -0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,  -0.   ,\n",
       "         0.   ,   0.   ,  -0.   ,   0.   ,  -0.   ,   0.   ,  -0.   ,\n",
       "        -0.   ,   0.   ,   0.   ,   0.   ,  -0.   ,  20.69 ,  -0.   ,\n",
       "        -0.   ,  -0.   ,  -0.   ,   0.   ,  -0.   ,  13.478,  -0.   ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After run model_experiment with complexity == 'polynomial'\n",
    "\n",
    "lr_ols = trained_models['ols']\n",
    "lr_lasso = trained_models['lasso']\n",
    "lr_ridge =trained_models['ridge']\n",
    "\n",
    "# check the coefficients from Lasso\n",
    "lr_lasso.coef_\n",
    "\n",
    "# compare them with OLS/Ridge models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.   , -30.236,  54.148,  55.111,   6.171,  -6.336,  -1.708,\n",
       "        -1.205,   0.146,  -0.838,  17.03 ,  -0.405, -15.06 , -12.86 ,\n",
       "       -13.26 ,  -3.213,  -0.809,   3.467,   4.689,  -1.136,   3.238,\n",
       "         6.103,  -4.399,  14.087,  13.417,   2.331,  -6.608,   8.873,\n",
       "       -10.393,  -9.342,  -1.623,  -4.457,   7.916,  12.786,   3.098,\n",
       "        -6.413,   8.786, -11.38 ,  -8.826,  -2.221,  -4.503,   8.91 ,\n",
       "         0.198,  -9.594,  -5.138,  -7.6  ,   1.017,  -9.075,  -2.76 ,\n",
       "         7.45 ,  -7.649,  -4.779,   2.775,   1.296,  -6.555,  -1.034,\n",
       "        -4.566,  -3.861,  -0.663,   0.021,   3.399,   3.456,  -1.37 ,\n",
       "        -1.205,   0.892,  -0.911,  -0.55 ,  -0.967,   0.146,   1.678,\n",
       "        -2.002,   2.978,  -0.838,   1.594,  -0.769,  17.03 ,  -2.031,\n",
       "         0.405, -14.247,  -4.62 ,  -5.055,   0.658,  -2.606,  -4.354,\n",
       "        10.55 ,   7.113,  -4.369,   3.404,   0.381,  -1.077,  -1.409,\n",
       "        -3.025,  -0.18 ,  -1.789,   4.932,   4.044,   3.075,   0.706,\n",
       "         0.009,  -1.782,  -3.257,  -0.012,  -1.241,   5.038,   4.176,\n",
       "         2.987,   0.261,  -0.58 , -10.337,   1.524,   0.28 ,   2.703,\n",
       "        -4.008,  -2.879,  -3.948,  -2.549, -22.919,   3.928,   2.074,\n",
       "         2.506,  12.909,   1.324,  -2.159, -19.381,   0.332,  -1.749,\n",
       "        -0.677,  -1.361,   1.857, -16.504,  10.391,  -6.18 ,  -7.049,\n",
       "         0.217, -18.36 ,  -6.364,   2.097,   0.83 , -30.198,   1.93 ,\n",
       "         0.869,  -2.913,   4.907, -27.341,   7.425,   7.364,  -2.584,\n",
       "        -1.084,   0.884,   0.477,   1.366,   3.506,  -5.927,   3.187,\n",
       "         7.34 ,  -1.764,  -0.923,   1.07 ,   0.476,   1.213,   3.667,\n",
       "        -6.088,   3.29 ,  14.766,   3.128,  -2.044,  -2.042,   0.9  ,\n",
       "         7.762,  -3.358,  -0.013,  23.817,  -5.45 ,  -0.974,  -0.164,\n",
       "        -4.6  ,  -2.343,  -0.459,  19.037,  -2.657,   0.487,   0.358,\n",
       "         0.866,   0.938,  27.85 ,  -8.073,  -2.543,   1.053,  -4.186,\n",
       "        25.1  ,  -0.24 ,  -0.972,  -2.491,  54.097,  -0.516,   1.878,\n",
       "        13.652,  -0.56 ,  46.71 ,   7.355,  -0.939,  -0.701,   1.279,\n",
       "         0.556,   0.999,   3.692,  -6.084,   3.353,  14.645,   3.765,\n",
       "        -1.869,  -0.434,  -0.015,   6.058,  -0.665,  -0.579,  25.058,\n",
       "        -4.547,  -1.818,   0.194,  -4.374,  -3.394,  -0.736,  19.653,\n",
       "        -3.054,   0.134,  -0.78 ,  -0.357,   2.838,  27.738,  -8.262,\n",
       "        -2.37 ,   1.399,  -2.119,  26.386,  -0.266,  -0.715,  -3.655,\n",
       "        55.056,  -2.141,   0.869,  13.598,   0.848,  47.026,   0.651,\n",
       "         2.6  ,  -2.366,  -0.409,  -5.488,  -2.299,  11.805,  -0.787,\n",
       "         7.908,   3.983,   2.731,   8.196,  11.592,  -2.208,  -3.137,\n",
       "         2.595,  -0.679,  11.644,   3.301,  -2.084,  -0.457,  -1.249,\n",
       "         2.186,  -3.378,   9.942,   6.604,   5.305,  -9.183,   8.746,\n",
       "        -0.758,   6.049,  -7.642,  -5.177,  -0.297,   9.6  ,   2.683,\n",
       "        -4.65 ,  -1.972,   1.317,  -0.6  ,  -5.896,   8.853,  -0.813,\n",
       "         2.025,   0.053,  -1.839,   7.262,   8.599,  -1.358,  -2.601,\n",
       "        -0.195,   6.331,  -0.792,   4.096,  -2.883,   2.572,  -7.527,\n",
       "         8.755,  -6.417,  -0.561,   8.465,  -3.657,   1.458,  -3.832,\n",
       "         1.588,   1.101,   2.062,  -5.581,   8.52 ,   1.392,  -1.642,\n",
       "         1.261,  -8.34 ,  -3.027,   3.906,  -1.088,   4.686,  -3.332,\n",
       "        -6.975,  -1.665,   3.021,   6.474,   2.632,  -3.391,  -0.988,\n",
       "        -1.205,   0.978,  -1.25 ,  10.082,  -1.004,  -0.576,  -0.268,\n",
       "        -5.258,  -1.503,  -1.218,  -4.261,   4.946,  -0.886,  -4.561,\n",
       "        -0.716,   0.146,   0.722,   8.87 ,   1.969,   0.167,  -4.125,\n",
       "        -2.145,  -1.813,   6.508,  -1.102,  -0.838,  17.083,  -0.415,\n",
       "         1.2  ,  -7.75 ,  -0.439,  17.03 ,  -2.026,  15.664,  -0.405])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the coefficients from Lasso\n",
    "lr_ridge.coef_\n",
    "\n",
    "# compare them with OLS/Ridge models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Effect of Scaling Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([51., 17., 12.,  8.,  5.,  6.,  0.,  0.,  0.,  1.]),\n",
       " array([-0.96 , -0.389,  0.182,  0.753,  1.324,  1.895,  2.466,  3.037,\n",
       "         3.608,  4.179,  4.75 ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.random.exponential(scale = 3, size = 100)\n",
    "\n",
    "plt.hist(x)\n",
    "\n",
    "\n",
    "x_scaled = (x - x.mean())/x.std(ddof = 1)\n",
    "\n",
    "\n",
    "plt.hist(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Normalization__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Integer subplot specification must be a three-digit number, not 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-65c1727ff8e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msubplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m     \u001b[0mbyebye\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m999\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                 raise ValueError(\"Integer subplot specification must be a \"\n\u001b[0;32m-> 1382\u001b[0;31m                                  \"three-digit number, not {}\".format(args[0]))\n\u001b[0m\u001b[1;32m   1383\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Integer subplot specification must be a three-digit number, not 1"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1)\n",
    "x_normalized = (x - x.mean())/np.sqrt(x.dot(x))\n",
    "plt.hist(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Effect of $\\alpha$ in Lasso and Ridge\n",
    "\n",
    "<img src=\"lasso_effect_of_lambda.png\" alt=\"Lasso-Lambda\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"ridge_effect_of_lambda.png\" alt=\"Lasso-Lambda\" style=\"width: 500px;\"/>\n",
    "\n",
    "<a name='questions'></a>\n",
    "\n",
    "\n",
    "#### Appendix\n",
    "<a name='appendix'></a>\n",
    "\n",
    "Here I would like to add some reading material that I found useful while working with the code.\n",
    "\n",
    "\n",
    "- [pd.get_dummies or OneHotEncoder? - Read second answer](https://stackoverflow.com/questions/36631163/pandas-get-dummies-vs-sklearns-onehotencoder-what-are-the-pros-and-cons)\n",
    "\n",
    "- [On dummy variable trap](https://www.algosome.com/articles/dummy-variable-trap-regression.html)\n",
    "\n",
    "- [sklearn.preprocessing.PolynomialFeatures documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
    "\n",
    "- [A great notebook on Lasso and Ridge](https://github.com/gokererdogan/JaverianaMLCourse/blob/master/Lectures/05.pdf)\n",
    "\n",
    "- [Another good blog post on Lasso and Ridge](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/)\n",
    "\n",
    "- Learn.co -- Section-28 Lasso-Ridge\n",
    "\n",
    "- [Toward Datascience Article](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)\n",
    "\n",
    "- [ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/) 2.2.2 The Bias-Variance Trade-off and 6.2 Shrinkage Methods\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Image Sources in order of appearance: \n",
    "- https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html\n",
    "\n",
    "- https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

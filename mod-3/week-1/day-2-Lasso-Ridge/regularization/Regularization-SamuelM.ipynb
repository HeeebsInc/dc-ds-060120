{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Content__\n",
    "\n",
    "- [Objectives](#objectives)\n",
    "\n",
    "- [Review](#review)\n",
    "\n",
    "- [Regularization Techniques](#regularization_techniques)\n",
    "\n",
    "- [Questions](#questions)\n",
    "\n",
    "- [Appendix](#appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objectives \n",
    "<a name=\"objectives\"></a>\n",
    "\n",
    "- Understand what is regularization\n",
    "\n",
    "- Understand the effect of hyper-parameter $\\alpha$ in Ridge and Lasso.\n",
    "\n",
    "- Understand the similarities and differences between Lasso-Ridge-Linear models.\n",
    "\n",
    "- Apply Lasso and Ridge with sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "<a name=\"review\"></a>\n",
    "\n",
    "\n",
    "\n",
    "[__Overfitting - Underfitting__](https://github.com/gokererdogan/JaverianaMLCourse/blob/master/Lectures/05.pdf)\n",
    "\n",
    "<img src=\"underfitting_overfitting.png\" alt=\"Bias-Variance\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "[__Bias - Variance Trade-Off__](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
    "\n",
    "<img src=\"bias_variance_trade_off.png\" alt=\"Bias-Variance\" style=\"width: 400px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preliminaries - L1 and L2 Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose we have a vector:  \n",
    "\n",
    "$$\n",
    "  \\begin{align}\n",
    "    X &= \\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           x_{3} \\\\\n",
    "           x_{4}\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then the square of the \"__L2-norm__\" of this vector is given by:\n",
    "\n",
    "$$ \\| X \\|^{2}_{2} = x_{1}^{2} + x_{2}^{2} + x_{3}^{2} +x_{4}^{2} = \\sum_{i=1}^{4} x_{i}^{2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using numpy:\n",
    "import numpy as np\n",
    "X = np.array([2, 0, 1, -1])\n",
    "l2_norm_squared = X.dot(X)\n",
    "l2_norm_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly the \"__L1-norm__\" of the X is given as:\n",
    "\n",
    "$$ \\| X \\|_{1} = \\lvert x_{1}\\rvert + \\lvert x_{2}\\rvert + \\lvert x_{3}\\rvert + \\lvert x_{4}\\rvert = \\sum_{i=1}^{4} \\lvert x_{i}\\rvert $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([2, 0, 1, -1])\n",
    "l1_norm = sum(np.abs(X))\n",
    "l1_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In two dimensions: If $$\n",
    "  \\begin{align}\n",
    "    X &= \\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\text{(Lasso) ---->} \\| X \\|_{1} = \\lvert x_{1}\\rvert + \\lvert x_{2}\\rvert = \\sum_{i=1}^{2} \\lvert x_{i}\\rvert $$\n",
    "\n",
    "$$\\text{(Ridge) ---->} \\| X \\|_{2}^{2} = \\lvert x_{1}\\rvert^{2} + \\lvert x_{2}\\rvert^{2} = \\sum_{i=1}^{2} \\lvert x_{i}\\rvert^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"norms.png\" alt=\"Lasso-Lambda\" style=\"width: 400px;\" class = \"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Examples: Note that if we have two vectors $ \\begin{align}\n",
    "    X &= \\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           x_{3} \\\\\n",
    "           x_{4}\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}$   and $ \\begin{align}\n",
    "    Y &= \\begin{bmatrix}\n",
    "           y_{1} \\\\\n",
    "           y_{2} \\\\\n",
    "           y_{3} \\\\\n",
    "           y_{4}\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}$ we can subtract them and get a new vector: $ \\begin{align}\n",
    "    X - Y &= \\begin{bmatrix}\n",
    "           x_{1} - y_{1} \\\\\n",
    "           x_{2} - y_{2}\\\\\n",
    "           x_{3} - y_{3}\\\\\n",
    "           x_{4} - y_{4}\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}$. Then now we can calculate the __L1-norm__ of the new vector $X-Y$ as:\n",
    "  \n",
    "  $$ \\| X-Y \\|_{1} = \\lvert x_{1} - y_{1} \\rvert + \\lvert x_{2} - y_{2} \\rvert + \\lvert x_{3} - y_{3} \\rvert + \\lvert x_{4} - y_{4} \\rvert = \\sum\\limits_{i=1}^{4} \\lvert x_{i} - y_{i} \\rvert$$\n",
    "  \n",
    "  Similarly the __L2-norm__ of this vector is given by:\n",
    "  \n",
    "  $$ \\| X-Y \\|_{2}^{2} =  (x_{1} - y_{1})^{2} + (x_{2} - y_{2} )^{2} + ( x_{3} - y_{3} )^{2} + ( x_{4} - y_{4} )^{2} =\\sum\\limits_{i=1}^{4} ( x_{i} - y_{i} )^{2} $$\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Regression Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Linear Model\n",
    "\n",
    "$$ Y = w_{0} + w_{1}X_1 + w_{2}X_{2} + \\cdots + w_{p}X_{p} + \\varepsilon $$\n",
    "\n",
    " - We train model to understand the paramaters $w_{i}$ (Coefficients) \n",
    "\n",
    " - $X_{i}$ 's are columns of the datasets (Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Once the coefficients are estimated (learned): We can make predictions for an observation (row) in our dataset:\n",
    "\n",
    " $$ \\hat{y}_{i} =  \\hat{w}_{0} + \\hat{w}_{1}X_{i1} + \\hat{w}_{2}X_{i2} + \\cdots + \\hat{w}_{p}X_{ip}$$\n",
    " \n",
    " - $\\hat{w}_{i}$'s are estimated coefficients\n",
    " - $X_{i1}, X_{i2}, \\cdots, X_{ip}$ - i'th row in the dataset (with p-columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    " Therefore for each observations (rows) errors are given by:\n",
    " \n",
    " $$ e_{i} = y_{i} - \\hat{y}_{i} $$\n",
    " \n",
    " As a result, the Residual Sum of Squares can be expressed as:\n",
    " \n",
    " $$ RSS(\\hat{w}_{0}, \\cdots, \\hat{w}_{p}) = \\sum\\limits_{i=0}^{N} e_{i}^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Usually the cost functions are denoted with the capital letter $J$\n",
    " \n",
    " \n",
    " $$ J(\\hat{w}_{0}, \\cdots, \\hat{w}_{p}) = \\sum\\limits_{i=0}^{N} (y_{i} - \\hat{w}_{0} - \\hat{w}_{1}X_{i1} - \\hat{w}_{2}X_{i2} - \\cdots - \\hat{w}_{p}X_{i_p})^{2} $$\n",
    " \n",
    " And this equation can be written in short hand as:\n",
    " \n",
    " $$ J(\\boldsymbol{w}) = \\| \\boldsymbol{Y} - X \\hat{w} \\|_{2}^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ridge regularization\n",
    "\n",
    "Instead of minimizing $J(w)$ (least squares method), we will minimize:\n",
    "\n",
    "$$ J_{\\alpha}(\\boldsymbol{w}) = J(\\boldsymbol{w}) + \\alpha\\sum_{i=1}^{p} w_{i}^{2} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Lasso regularization\n",
    "\n",
    "Instead of minimizing $J(\\boldsymbol{w})$, we will minimize:\n",
    "\n",
    "$$ J_{\\alpha}(\\boldsymbol{w}) = J(\\boldsymbol{w}) + \\alpha\\sum_{i=1}^{p}| w_{i} | $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularization Techniques\n",
    "\n",
    "<a name=\"regularization_techniques\"></a>\n",
    "\n",
    "\n",
    "- Why?\n",
    "\n",
    "    - Reduces complexity\n",
    "    \n",
    "    - Reduce the chance of overfitting.\n",
    "    \n",
    "    - Reduces model's variance at the expense of introducing small bias\n",
    "    \n",
    "    - Increases model's interprettability.\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "df = pd.read_csv('../data/Credit.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.891</td>\n",
       "      <td>3606</td>\n",
       "      <td>283</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>106.025</td>\n",
       "      <td>6645</td>\n",
       "      <td>483</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Asian</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104.593</td>\n",
       "      <td>7075</td>\n",
       "      <td>514</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>148.924</td>\n",
       "      <td>9504</td>\n",
       "      <td>681</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55.882</td>\n",
       "      <td>4897</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(400, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see the head of the dataset\n",
    "display(HTML(df.head().to_html()))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Brief Investigation of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 400 entries, 1 to 400\n",
      "Data columns (total 11 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Income     400 non-null    float64\n",
      " 1   Limit      400 non-null    int64  \n",
      " 2   Rating     400 non-null    int64  \n",
      " 3   Cards      400 non-null    int64  \n",
      " 4   Age        400 non-null    int64  \n",
      " 5   Education  400 non-null    int64  \n",
      " 6   Gender     400 non-null    object \n",
      " 7   Student    400 non-null    object \n",
      " 8   Married    400 non-null    object \n",
      " 9   Ethnicity  400 non-null    object \n",
      " 10  Balance    400 non-null    int64  \n",
      "dtypes: float64(1), int64(6), object(4)\n",
      "memory usage: 37.5+ KB\n"
     ]
    }
   ],
   "source": [
    "## checking for missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Caucasian           199\n",
       "Asian               102\n",
       "African American     99\n",
       "Name: Ethnicity, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## let's see the number of categories in ethnicity\n",
    "df.Ethnicity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Female    207\n",
       " Male     193\n",
       "Name: Gender, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     360\n",
       "Yes     40\n",
       "Name: Student, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Student.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yes    245\n",
       "No     155\n",
       "Name: Married, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Married.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns = \"Balance\"),\n",
    "                                                df.Balance, \n",
    "                                                stratify = df.Married,\n",
    "                                                random_state =42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding the Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## instantiate ohe\n",
    "## note that in this case we know the categories very well.\n",
    "ohe = OneHotEncoder(categories= [[\"Caucasian\", \"Asian\", \"African American\"], [\"Female\", \" Male\"], [\"No\", \"Yes\"], [\"Yes\", \"No\"]],\n",
    "                    drop = [\"Caucasian\", \"Female\", \"No\", \"No\"], sparse = False,)\n",
    "\n",
    "## fit model (learn categories in train and plan how to convert them one hot encoded columns)\n",
    "ohe.fit(X_train[['Ethnicity', \"Gender\", \"Student\", \"Married\"]])\n",
    "## transform X_train data. Result is a numpy array.\n",
    "categorical_train = ohe.transform(X_train[['Ethnicity', \"Gender\", \"Student\", \"Married\"]])\n",
    "## concact the categorical variables with the rest of the data\n",
    "train = np.hstack((X_train.select_dtypes(exclude = \"object\" ), categorical_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 11)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.9656e+01, 8.2440e+03, 5.7900e+02, ..., 1.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00],\n",
       "       [1.1603e+01, 2.2780e+03, 1.8700e+02, ..., 1.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00],\n",
       "       [7.5257e+01, 7.0100e+03, 4.9400e+02, ..., 0.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00],\n",
       "       ...,\n",
       "       [2.3350e+01, 2.5580e+03, 2.2000e+02, ..., 0.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [2.6400e+01, 5.6400e+03, 3.9800e+02, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [4.3682e+01, 6.9220e+03, 5.1100e+02, ..., 1.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_test = ohe.transform(X_test[['Ethnicity', \"Gender\", \"Student\", \"Married\"]])\n",
    "test = np.hstack((X_test.select_dtypes(exclude = \"object\"), categorical_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 11)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0_Asian', 'x0_African American', 'x1_ Male', 'x2_Yes', 'x3_Yes'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import \n",
    "from sklearn.linear_model import LinearRegression\n",
    "## instantiate\n",
    "lr = LinearRegression()\n",
    "## fit \n",
    "lr.fit(train, y_train) #here ML happens, minimizing the cost (predicting coefficients)\n",
    "## predict\n",
    "y_train_pred = lr.predict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -7.68 ,   0.2  ,   0.952,  15.031,  -0.651,  -1.589,   5.257,\n",
       "         0.649,   8.267, 434.605,  -3.351])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True, precision=3)\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9535410834118568"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## R_squared score\n",
    "lr.score(train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import\n",
    "from sklearn.linear_model import Ridge\n",
    "#the difference between Ridge and linear is the cost function \n",
    "## instantiate\n",
    "ridge = Ridge(alpha = 8, normalize = True)\n",
    "## fit\n",
    "ridge.fit(train, y_train)\n",
    "## predict\n",
    "ridge_train_pred = ridge.predict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.474,  0.017,  0.247,  4.012, -0.083,  0.125,  0.036,  6.281,\n",
       "       -4.822, 49.14 ,  0.405])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## coefficients\n",
    "ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06712349115367588"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## score\n",
    "ridge.score(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-197cb393f305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBalance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Shape of the dataset is:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "X = df2.values\n",
    "y = df.Balance.values.reshape(-1,1)\n",
    "\n",
    "print('Shape of the dataset is:', X.shape)\n",
    "\n",
    "\n",
    "## Create higher order terms\n",
    "poly = PolynomialFeatures(degree=4)\n",
    "Xp = poly.fit_transform(X)\n",
    "\n",
    "print('After adding higher order terms: ', Xp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_experiment(num_iter = 5, \n",
    "                     models = ['ols', 'ridge', 'lasso'], alpha= 10, \n",
    "                     complexity = 'simple', degree = 3):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    _________________________\n",
    "    num_iter: int, number of times fit the models to test data each time with different splitting. \n",
    "    note that for each iteration we split the data random train and test parts.\n",
    "    models: list, list of models that we want to use. Options are 'ols' for simple linear regression\n",
    "    'ridge' for ridge regression and 'lasso' for lasso regression.\n",
    "    alpha: float, alpha parameter for ridge and lasso algorithms. Recall that higher values of alpha \n",
    "    leads to more regularization.\n",
    "    complexity: str, either 'simple' or 'polynomial'. We either use the original dataset or \n",
    "    a dataset with polynomial powers generated. \n",
    "    degree: int, if complexity is polynomial then degree is the degrees of polynomials to be generated.\n",
    "    return: dict, it returns a dictionary with trained models as values and the 'complexity' parameters as keys.\n",
    "    \"\"\"\n",
    "    \n",
    "    x_axis = np.arange(num_iter)\n",
    "    y_ols_test = []\n",
    "    y_lasso_test = []\n",
    "    y_ridge_test = []\n",
    "    sample_models = {}\n",
    "    for i in range(num_iter):\n",
    "        \n",
    "        if complexity == 'simple':\n",
    "            ## split train_test \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "        elif complexity == 'polynomial':\n",
    "            ## Create higher order terms\n",
    "            poly = PolynomialFeatures(degree=degree)\n",
    "            Xp = poly.fit_transform(X)\n",
    "            ## test-train split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(Xp, y, test_size = 0.2)\n",
    "\n",
    "\n",
    "        ## Standard scale mean = 0, variance = 1\n",
    "        sd = StandardScaler()\n",
    "\n",
    "        sd.fit(X_train)\n",
    "\n",
    "        X_train = sd.transform(X_train)\n",
    "\n",
    "        X_test = sd.transform(X_test)\n",
    "\n",
    "        ## Be careful about the leakage\n",
    "\n",
    "        ## Vanilla model\n",
    "        if 'ols' in models:\n",
    "            lr = LinearRegression()\n",
    "\n",
    "            lr.fit(X_train, y_train)\n",
    "            \n",
    "            sample_models['ols'] = lr\n",
    "\n",
    "            test_score = lr.score(X_test, y_test)\n",
    "            train_score = lr.score(X_train, y_train)\n",
    "\n",
    "            y_ols_test.append(test_score)\n",
    "\n",
    "    #       print('test score OLS is %.2f and train score is %.2f'%(test_score, train_score))\n",
    "\n",
    "        if 'ridge' in models:\n",
    "            ## Ridge in the simple setting\n",
    "            ridge = Ridge(alpha = alpha, max_iter= 10000)\n",
    "            ridge.fit(X_train, y_train)\n",
    "            sample_models['ridge'] = ridge\n",
    "            y_ridge_test.append(ridge.score(X_test, y_test))\n",
    "    #         print('test score Ridge is %.2f and train score is %.2f'%(ridge.score(X_test, y_test),\n",
    "    #                                                             ridge.score(X_train, y_train)))\n",
    "\n",
    "        if 'lasso' in models:\n",
    "            ## Lasso in the simple setting\n",
    "            lasso = Lasso(alpha = alpha, max_iter= 10000)\n",
    "\n",
    "            lasso.fit(X_train, y_train)\n",
    "            \n",
    "            sample_models['lasso'] = lasso\n",
    "            \n",
    "            y_lasso_test.append(lasso.score(X_test, y_test))\n",
    "    #       print('test score Lasso is %.2f and train score is %.2f'%(lasso.score(X_test, y_test),\n",
    "    #                                                             lasso.score(X_train, y_train)))\n",
    "\n",
    "        i+=1\n",
    "    if 'ols' in models:\n",
    "        plt.plot(y_ols_test, label = 'ols')\n",
    "    if 'ridge' in models:\n",
    "        plt.plot(y_ridge_test, label = 'ridge')\n",
    "    if 'lasso' in models:\n",
    "        plt.plot(y_lasso_test, label = 'lasso')\n",
    "    plt.ylabel('R2 test score')\n",
    "    plt.xlabel('number of iterations')\n",
    "    plt.ylim((0.50, 0.99))\n",
    "    \n",
    "    plt.legend()\n",
    "    return sample_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-f9cb7badad8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m trained_models = model_experiment(num_iter=10, alpha = 100,\n\u001b[1;32m      2\u001b[0m                                    \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ols'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ridge'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lasso'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                    complexity= 'polynomial', degree = 3)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-921ace035d0d>\u001b[0m in \u001b[0;36mmodel_experiment\u001b[0;34m(num_iter, models, alpha, complexity, degree)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m## Create higher order terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mpoly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolynomialFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mXp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;31m## test-train split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "trained_models = model_experiment(num_iter=10, alpha = 100,\n",
    "                                   models = ['ols', 'ridge', 'lasso'], \n",
    "                                   complexity= 'polynomial', degree = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn__\n",
    "\n",
    "- Try different values for alpha --> report your observations\n",
    "\n",
    "- Change complexity = 'polynomial' and observe the change in the variance of the models. \n",
    "\n",
    "- Report your observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.       ,  -0.       ,   0.       , 286.0867823,   0.       ,\n",
       "        -0.       ,   0.       ,  -0.       ,   0.       ,   0.       ,\n",
       "        -0.       ,  -0.       ,  -0.       ,   0.       ,   0.       ,\n",
       "         0.       ,   0.       ,   0.       ,   0.       ,   0.       ,\n",
       "         0.       ,   0.       ,   0.       ,   0.       ,   0.       ,\n",
       "        -0.       ,  -0.       ,   0.       ,  -0.       ,  -0.       ,\n",
       "        -0.       ,  -0.       ,  -0.       ,  -0.       ,   0.       ,\n",
       "         0.       ,   0.       ,   0.       ,   0.       ,   0.       ,\n",
       "         0.       ,   0.       ,   0.       ,   0.       ,  -0.       ,\n",
       "         0.       ,  -0.       ,  -0.       ,  -0.       ,   0.       ,\n",
       "         0.       ,   0.       ,   0.       ,   0.       ,   0.       ,\n",
       "         0.       ,   0.       ,   0.       ,   0.       ,   0.       ,\n",
       "         0.       ,   0.       ,   0.       ,   0.       ,   0.       ,\n",
       "         0.       ,   0.       ,   0.       ,   0.       ,   0.       ,\n",
       "         0.       ,   0.       ,   0.       ,   0.       ,   0.       ,\n",
       "         0.       ,   0.       ,  -0.       ,   0.       ,   0.       ,\n",
       "        -0.       ,  -0.       ,  -0.       ,   0.       ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After run model_experiment with complexity == 'polynomial'\n",
    "\n",
    "lr_ols = trained_models['ols']\n",
    "lr_lasso = trained_models['lasso']\n",
    "lr_ridge =trained_models['ridge']\n",
    "\n",
    "# check the coefficients from Lasso\n",
    "lr_lasso.coef_\n",
    "\n",
    "# compare them with OLS/Ridge models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        , -18.53671764,  58.17965901,  58.13723906,\n",
       "          4.54738655,  -5.59933032,  -5.15621323, -10.53571011,\n",
       "         -2.22807612,  -3.18678826, -19.4492761 , -15.30686841,\n",
       "        -20.54812509,  40.04446374,  40.53535373,  31.01229096,\n",
       "         24.59923782,  36.38210854,  40.92118213,  28.86778144,\n",
       "         23.03666327,  35.37217786,   1.12471953,  -2.07770367,\n",
       "         -4.31984938,  -7.97920737,  -4.23494998,  -8.11417243,\n",
       "         -4.96584507, -10.73875654, -10.85480737, -16.5268444 ,\n",
       "        -10.12311915, -20.19321127,  -8.84586239,  -8.76556271,\n",
       "        -12.47549687,  -9.9736308 , -12.08409193,  -8.73323249,\n",
       "        -13.07000555, -10.42205609, -12.55996344, -20.52297437,\n",
       "        -18.82805836, -24.48162284, -16.2309472 , -18.53879987,\n",
       "        -20.65215732,  12.1027276 ,  13.07766793,  19.38848078,\n",
       "         13.15341367,  21.59703029,  14.02594602,  19.49052099,\n",
       "         13.14900208,  21.97739225,  10.6749634 ,  10.08208595,\n",
       "         13.33231861,   2.46978937,  11.52977493,  19.3691719 ,\n",
       "         14.94113403,  19.47349681,  13.04933903,  22.27211288,\n",
       "          9.27093584,   8.60953187,  11.88683183,   1.22412251,\n",
       "         10.38139003,  17.85221925,  -3.55515776,  -4.45374085,\n",
       "         -5.8381558 ,  -8.70482277,  -7.88453283, -12.33604487,\n",
       "        -10.25350812,  -6.69385304,  -5.09083307, -11.28155909]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the coefficients from Lasso\n",
    "lr_ridge.coef_\n",
    "\n",
    "# compare them with OLS/Ridge models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Effect of Scaling Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([43., 20., 13., 10.,  5.,  4.,  2.,  0.,  2.,  1.]),\n",
       " array([-0.98383431, -0.50391153, -0.02398875,  0.45593403,  0.93585681,\n",
       "         1.41577959,  1.89570237,  2.37562514,  2.85554792,  3.3354707 ,\n",
       "         3.81539348]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.random.exponential(scale = 3, size = 100)\n",
    "\n",
    "plt.hist(x)\n",
    "\n",
    "\n",
    "x_scaled = (x - x.mean())/x.std(ddof = 1)\n",
    "\n",
    "\n",
    "plt.hist(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Effect of $\\alpha$ in Lasso and Ridge\n",
    "\n",
    "<img src=\"lasso_effect_of_lambda.png\" alt=\"Lasso-Lambda\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"ridge_effect_of_lambda.png\" alt=\"Lasso-Lambda\" style=\"width: 500px;\"/>\n",
    "\n",
    "<a name='questions'></a>\n",
    "### Questions\n",
    "\n",
    "\n",
    "\n",
    "Q. Should I do normalization for Lasso or Ridge?\n",
    "\n",
    "A. Yes? Why?\n",
    "\n",
    "Q. When we know that Ridge and Lasso is better than vanilla linear regression?\n",
    "\n",
    "A. High variation in your model --> Colinearity and too many variables.\n",
    "\n",
    "Q. How do we know whether we should choose Lasso or Ridge?\n",
    "\n",
    "A. Most of the time they perform very similar but Lasso has the feature selection property, ridge doesn't have this.\n",
    "\n",
    "Q: How do we choose $\\lambda$?\n",
    "\n",
    "A. [sklearn gridsearch](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) for small models or random grid search for bigger models.\n",
    "\n",
    "#### Appendix\n",
    "<a name='appendix'></a>\n",
    "\n",
    "Here I would like to add some reading material that I found useful while working with the code.\n",
    "\n",
    "\n",
    "- [pd.get_dummies or OneHotEncoder? - Read second answer](https://stackoverflow.com/questions/36631163/pandas-get-dummies-vs-sklearns-onehotencoder-what-are-the-pros-and-cons)\n",
    "\n",
    "- [On dummy variable trap](https://www.algosome.com/articles/dummy-variable-trap-regression.html)\n",
    "\n",
    "- [sklearn.preprocessing.PolynomialFeatures documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
    "\n",
    "- [A great notebook on Lasso and Ridge](https://github.com/gokererdogan/JaverianaMLCourse/blob/master/Lectures/05.pdf)\n",
    "\n",
    "- [Another good blog post on Lasso and Ridge](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/)\n",
    "\n",
    "- Learn.co -- Section-28 Lasso-Ridge\n",
    "\n",
    "- [Toward Datascience Article](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)\n",
    "\n",
    "- [ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/) 2.2.2 The Bias-Variance Trade-off and 6.2 Shrinkage Methods\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Image Sources in order of appearance: \n",
    "- https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html\n",
    "\n",
    "- https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
